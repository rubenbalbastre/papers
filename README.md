# üìö Papers I‚Äôve Read

A running list of papers I‚Äôve read (and am reading) with brief notes ‚Äî mainly on AI and related topics.
I started this in October 2025 and update it regularly.

---

## üóÇÔ∏è Structure
Each entry includes:
- **Title**
- **Link** (to arXiv, journal, or PDF)
- **Tags** (topics or methods)
- **Notes**

---

## üß† 2025

### üî∏ Embeddings

| Title | Tags | Notes |
| -- | -- | -- |
| [Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models](https://arxiv.org/pdf/2506.05176) | `Embeddings`, `LLM`, `QWEN`, `ReRankers` |  |

---

### üî∏ Large Language Models

| Title | Tags | Notes |
| -- | -- | -- |
| [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783) | `LLM`, `LLAMA 3` | General understanding of training stages |
| [Why Language Models Hallucinate](https://arxiv.org/pdf/2509.04664) | `Hallucination` | not too bad |
| [RL‚ÄôS RAZOR: WHY ONLINE REINFORCEMENT LEARNING FORGETS LESS](https://arxiv.org/pdf/2509.04259)| `Hallucination` | RL vs SFT forgettting |
|[Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](https://arxiv.org/pdf/2405.05904)|`Hallucination` | MUST read it |
|[The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/pdf/2506.06941)| `Reasoning` | Conclusions not too fair but interesting |
---

### üî∏ LLM Agents

| Title | Tags | Notes |
| -- | -- | -- | 
| [AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions](https://arxiv.org/pdf/2508.11152) | `LLM Multi-Agent Systems` | inspiration |
| [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/pdf/2510.04618)| `LLM Multi-Agent`, `Context Engineering` | |

---

### üî∏ AI Architectures
| Title | Tags | Notes |
| -- | -- | -- |
| [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/pdf/2506.09985) | `JEPA`, `Vision` | application of JEPA |
| [CWM: An Open-Weights LLM for Research on Code Generation with World Models](https://arxiv.org/pdf/2510.02387) | `JEPA`, `Code Generation` | application of JEPA |
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | `Transformers`, `Attention` | Transformers |

---

### üî∏ AI General
| Title |Tags | Notes |
|-------|------|----------|
| [A Path Towards Autonomous Machine Intelligence](https://arxiv.org/abs/2402.17193) | `JEPA`, `Representation Learning` | JEPA predicts embeddings rather than sensory data, offering a scalable self-supervised framework. |
| [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) | `Scaling`, `Transformers` | Demonstrates predictable performance scaling with model size, dataset size, and compute. |
| [The Art of Scaling Reinforcement Learning Compute for LLMs](https://arxiv.org/html/2510.13786v1#S1) | `RL` | need to deep dive |

---

### üî∏ Physics-Informed Machine Learning
| Title |  Tags | Notes |
|-------|------|----------|
| [Learning orbital dynamics of binary black hole systems from gravitational wave measurements](https://arxiv.org/pdf/2102.12695) | `PINN`, `PDE`, `Inverse Problems` | PINNs embed PDE residuals into the loss function to enforce physical consistency. |
---
